---
title: "Topic_models"
author: "Harsh Sharda"
date: "4/18/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(quanteda)
library(SpeedReader)
library(slam)
library(tidyverse)
library(ggplot2)
library(stm)
```

```{r}
# Loading Data
# A file with 205 documents
load("data/base_data.Rdata")
```

```{r}
#Adding categories
# Articles written before and after Trump election
base_data$election <- ifelse(base_data$year>=2017,"post_trump","pre_trump")
sapply(base_data,class)

# Converting factor into character
i <- sapply(base_data, is.factor)
base_data[i] <- lapply(base_data[i], as.character)
sapply(base_data,class)
```

```{r}
# Removing unncessary characters
term_topic <- base_data %>%
  mutate(text = stringr::str_replace_all(text,"-"," ")) %>%
  mutate(text = stringr::str_replace_all(text,">"," ")) %>%
  mutate(text = stringr::str_replace_all(text,"<"," ")) %>%
  mutate(text = stringr::str_replace_all(text,"$"," ")) %>%
  mutate(text = stringr::str_replace_all(text,"&#x27;","'")) %>%
  mutate(text = stringr::str_replace_all(text,"&amp;","&")) %>%
  mutate(text = stringr::str_replace_all(text,"&quot;","")) %>%
  mutate(text = stringr::str_replace_all(text,"/li","")) %>%
  mutate(text = stringr::str_replace_all(text," li ","")) %>%
  mutate(text = stringr::str_replace_all(text,"/ol","")) %>%
  mutate(text = stringr::str_replace_all(text," ol ","")) %>%
  
  #Removing authors that are coming in the terms
  #mutate(text = stringr::str_replace_all(text,"Krutilla","")) %>%
  #mutate(text = stringr::str_replace_all(text,"Pinchot","")) %>%
  #mutate(text = stringr::str_replace_all(text,"Muir","")) %>%
  
  mutate(text = stringr::str_replace_all(text,"<(/|)ol>"," ")) %>%
  mutate(text = stringr::str_replace_all(text,"<(/|)li>"," "))
```



```{r}
# Creating a dataframe object
corpus_topic <- data.frame(sno = paste(1:204),
                          text = term_topic[["text"]],
                          date = term_topic[["date.x"]],
                          topic = term_topic[["topic"]],
                          topic_merge = term_topic[["topic_merge"]],
                          election = term_topic[["election"]],
                          year = term_topic[["year"]],
                          stringsAsFactors = FALSE)
```

```{r}
# Creating a corpus object
corpus_topic_final <- corpus(corpus_topic,
                          docid_field = "sno",
                          text_field = "text",
                          metacorpus = list(source = "204 Articles RFF"))
summary(corpus_topic_final)
```

```{r}
############### Topic Models using STM package #################

# create a dfm object
dtm <- dfm(corpus_topic_final,
           remove_punct = TRUE,
           remove_numbers = TRUE,
           remove = stopwords("english"))

# look at number of features:
dtm
```

```{r}
# in order to do some preliminary checks, lets convert our quanteda dfm object
# into an stm object. In general, we do not need to do this, but in this
# instance it will let us make a cool plot:
check <-quanteda::convert(dtm,
                          to = "stm")
```


```{r}
# we can make a plot to see how removing terms appearing in less than x documents
# would affect or corpus:
png(file = "~/Desktop/Removing_Terms_RFF.png",
    width = 9,
    height = 3,
    units = "in",
    res = 200)
plotRemoved(check$documents,
            lower.thresh=seq(1,200, by=50))
dev.off()
```

```{r}
summary(rowSums(dtm))
```

```{r}
# lets fit a simple topic model with twenty topics, and the default
# hyperparameters using the STM package. The stm package will fit this topic
# model using a variational approximation which is different from the collapsed
# Gibbs sampler we saw in the lecture, but effectively does the same thing:
lda_fit <- stm(dtm,
               K = 15,
               seed = 12345,
               verbose = TRUE)
```


```{r}
# we can start by looking at the top 8 terms in each of the topics,
# along with thier overall proportion in the corpus:
png(file = "~/Desktop/Topic_Summaries_RFF.png",
    width = 12,
    height = 7,
    units = "in",
    res = 200)
plot.STM(lda_fit,
         type="summary",
         n = 8)
dev.off()
```

```{r}
# we can also pull out documents that are most highly associated associated
# with a topic:
findThoughts(lda_fit,
             texts = texts(corpus_topic_final),
             topics = 8,
             n = 1)
```


```{r}
# now lets trim the vocabulary to make things easier to work with:
dtm <- dfm_trim(dtm,
                min_termfreq = 5)
```

```{r}
summary(rowSums(dtm))
```

```{r}
# lets fit a simple topic model with twenty topics, and the default
# hyperparameters using the STM package. The stm package will fit this topic
# model using a variational approximation which is different from the collapsed
# Gibbs sampler we saw in the lecture, but effectively does the same thing:
lda_fit <- stm(dtm,
               K = 10,
               seed = 12345,
               verbose = TRUE)
```

```{r}
# we can start by looking at the top 8 terms in each of the topics,
# along with thier overall proportion in the corpus:
png(file = "~/Desktop/Topic_Summaries_RFF.png",
    width = 12,
    height = 7,
    units = "in",
    res = 200)
plot.STM(lda_fit,
         type="summary",
         n = 8)
dev.off()
```


```{r}
# we can also pull out documents that are most highly associated associated
# with a topic:
findThoughts(lda_fit,
             texts = texts(corpus_topic_final),
             topics = 10,
             n = 1)
```

```{r}
################ Working with a larger corpus in MALLET ##############
# The corpus is getting larger by essentially using ngrams of 1:3
dtm <- dfm(corpus_topic_final,
           remove_punct = TRUE,
           remove = stopwords("english"))

dtm <- tokens_remove(tokens(corpus_topic_final, remove_punct = TRUE), stopwords("english")) %>% tokens_ngrams(., n = 1:3) %>% dfm(.)

dtm
```


```{r}
summary(rowSums(dtm))
```

```{r}
dtm <- dfm_trim(dtm,
                min_termfreq = 5)
```

```{r}
# now we convert to a simple triplet matrix
dtm_trip <- SpeedReader::convert_quanteda_to_slam(dtm)

# Lets try running a topic model using mallet:
# http://mallet.cs.umass.edu/topics.php
# This implementation is much more scalable than the version in stm, but
# requires a bit more legwork to get it working

setwd("~/Desktop")
lda_mallet <- mallet_lda(
    documents = dtm_trip,
    topics = 10,
    iterations = 300,
    burnin = 10,
    alpha = 1,
    beta = 0.01,
    hyperparameter_optimization_interval = 20,
    cores = 1,
    delete_intermediate_files = FALSE)

# lets look at the top words in each topic:
topic_output <- cbind(lda_mallet$topic_metadata,
                      lda_mallet$topic_top_words)

write.csv(topic_output, "data/topic_output_1.csv")

```

```{r}
# we can see how our results would change is we were to use ACMI preprocessing:
# start by pulling out the document covariates
document_covariates <- docvars(dtm)
# generate a contingency table:
topic_party_table <- contingency_table(
    metadata = document_covariates,
    document_term_matrix = dtm_trip,
    variables_to_use = c("topic_merge"),
    threshold = 10
)

# get ACMI contributions:
acmi_contribs <- ACMI_contribution(topic_party_table)

# remove all the negative contribution terms:
dtm_acmi <- dtm_trip[,-acmi_contribs$negative_vocab]

# now find the documents that no longer have any words in them:
rem <- which(slam::row_sums(dtm_acmi) == 0)

# remove these documents:
docvars_acmi <- document_covariates[-rem,]
dtm_acmi <- dtm_acmi[-rem,]
```

```{r}
setwd("~/Desktop")
lda_acmi <- mallet_lda(
    documents = dtm_acmi,
    topics = 10,
    iterations = 500,
    burnin = 10,
    alpha = 1,
    beta = 0.01,
    hyperparameter_optimization_interval = 20,
    cores = 1,
    delete_intermediate_files = TRUE)

# lets look at the top words in each topic:
topic_output_acmi <- cbind(lda_acmi$topic_metadata,
                           lda_acmi$topic_top_words)

write.csv(topic_output_acmi, "data/topic_output_2.csv")

```

```{r}
kwic(corpus_topic_final,"kyoto")
base_data[["text"]][26]
```

