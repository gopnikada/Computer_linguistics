---
title: "Untitled"
author: "Harsh Sharda"
date: "1/25/2020"
output: html_document
---

```{r setup, include=FALSE}
# Preliminaries
rm(list = ls())

# Set your working directory to some place you can find
setwd("~/Semester 4/Text as Data/Computer_linguistics")
```


```{r}
# httr is a package for downloading html
library(httr)
# A package for manipulating strings
library(stringr)
library(XML)
library(rvest)
library(tidyverse)
```


# Going through the 20 pages of the website and scraping data

```{r}
i <- c(1:20)
links <- vector()
for(val in i){
  url2 <- paste0(url,"?page=",val)
  page <- httr::GET(url2)
  str(page)
  page_content <- httr::content(page, "text")
  text_split_content <- trimws(stringr::str_split(page_content,"\\n")[[1]])
  urls <- stringr::str_match(text_split_content,"href=\"/archives/(.+)\"")[,2]
  urls <- urls[!is.na(urls)]
  links <- append(links, urls[3:length(urls)])
}
```

## Creating a dataframe for storing all the information

```{r}
data <- data.frame(url = links,
                   text_title = rep("",length(links)),
                   text_content = rep("",length(links)),
                   authors = rep("",length(links)),
                   text = rep("",length(links)),
                   date = rep("",length(links)),
                   stringsAsFactors = FALSE)
```


# Creating a loop for the extraction process

```{r}
# loop through urls, download them, extract information:
for (i in 301:420) {
  
  link <- httr::GET(paste0(url,links[i]))
  text <- httr::content(link, "text")
  data$text_title[i] <- stringr::str_match(text,"<title>(.+)</title>")[,2]
  data$text_content[i] <- stringr::str_match(text,"content=\"(.+).\"")[,2]
  auth_check <- stringr::str_match_all(text,"<h3 class=\"author__name\">(.*)</h3>")[[1]][,2]
  
  if (length(auth_check)>0){
    data$authors[i] <- auth_check
  }
  
  text <- stringr::str_extract_all(text,"<div class=\"rich-text\">.*</div>")[[1]]
  
  raw = read_html(link)
  
  if (length(text)==0){
    text = raw %>% html_nodes(.,xpath="/html/body/main/div/div[2]/section/div[1]/div") %>%
      html_text()
    text <- stringr::str_replace_all(text,"\n","")
  } else {
    text <- unlist(stringr::str_extract_all(text,"<p>(.*)</p>"))
    text <- paste(unlist(stringr::str_split(text,"\\<(/|)p>")),collapse = "")
    text <- stringr::str_replace_all(text,"<(/|)i>", "")
    text <- stringr::str_replace_all(text,"<(/|)b>", "")
    text <- stringr::str_replace_all(text,"<(/|)h3>", "")
    text <- stringr::str_replace_all(text,"039", "'")
    text <- stringr::str_replace_all(text,"<a href=\"(.+?)\">","")
    text <- stringr::str_replace_all(text,"</a>","")
    }
  
  data$text[i] <- text
  
  data$date[i] = raw %>% html_nodes(.,xpath="/html/body/main/div/div[1]/div/div[2]/p[1]") %>% html_text() %>% trimws(.) %>% substr(1,8)

  #make sure we are not downloading data too fast
  Sys.sleep(2)
  
}
```

## Intermediate Codes:

```{r}
url <- "https://www.resourcesmag.org/archives/"
page <- httr::GET(url)
str(page)
page_content <- httr::content(page, "text")
cat(page_content)

# and write it to a file for easier viewing
write.table(x = page_content,
            col.names = FALSE,
            row.names = FALSE,
            quote = FALSE,
            file = "Links_search.html")
```

```{r}
text_split_content <- trimws(stringr::str_split(page_content,"\\n")[[1]])
text_split_content

# "href=\"/archives/how-clean-is-refined-coal/\""
stringr::str_extract_all(text_split_content,"href=\"/archives/(.+)/")
need  <- stringr::str_match(text_split_content,"href=\"/archives(.+)\"")[,2]
need[!is.na(need)]
```

```{r}
require(rvest)
require(tidyverse)
raw = read_html("https://www.resourcesmag.org/archives/protecting-our-national-parks-new-entrance-fees-can-help/")

link1 <- httr::GET(paste0(url,links[3]))
raw = read_html(link1)
body = raw %>% html_nodes(.,xpath="/html/body/main/div/div[2]/section/div[1]/div") %>% html_text()
date = raw %>% html_nodes(.,xpath="/html/body/main/div/div[1]/div/div[2]/p[1]") %>% html_text() %>% trimws(.) %>% substr(1,8)



```

# Queries for extracting relevant information from the webpage

```{r}
link1 <- httr::GET(paste0(url,links[285]))
text <- httr::content(link1, "text")

write.table(x = text,
            col.names = FALSE,
            row.names = FALSE,
            quote = FALSE,
            file = "content.html")

text_title <- stringr::str_match(text,"<title>(.+)</title>")[,2]
text_content <- stringr::str_match(text,"content=\"(.+).\"")[,2]


#text_split_content <- trimws(stringr::str_split(text,"\\n")[[1]])
#text_split_content

text <- stringr::str_extract_all(text,"<div class=\"rich-text\">.*</div>")[[1]]


if (length(text)==0){
  raw = read_html(link1)
  text = raw %>% html_nodes(.,xpath="/html/body/main/div/div[2]/section/div[1]/div") %>%
    html_text()
  text <- stringr::str_replace_all(text,"\n","")
} else {
  text <- unlist(stringr::str_extract_all(text,"<p>(.*)</p>"))
  text <- paste(unlist(stringr::str_split(text,"\\<(/|)p>")),collapse = "")
  text <- stringr::str_replace_all(text,"<(/|)i>", "")
  text <- stringr::str_replace_all(text,"<(/|)b>", "")
  text <- stringr::str_replace_all(text,"<(/|)h3>", "")
  text <- stringr::str_replace_all(text,"039", "'")
  text <- stringr::str_replace_all(text,"<a href=\"(.+?)\">","")
  text <- stringr::str_replace_all(text,"</a>","")
  }

a <- stringr::str_match_all(text,"<h3 class=\"author__name\">(.*)</h3>")

if (length(a)>0){
  authors <- a[[1]][2]
}
# <h3 class=\"author__name\">James N. Sanchirico</h3>
authors <- stringr::str_match_all(text,"<h3 class=\"author__name\">(.*)</h3>")[[1]][,2]
```


```{r}
# loop through urls, download them, extract information:
for (i in 50:length(links)) {
  
  link <- httr::GET(paste0(url,links[i]))
  text <- httr::content(link, "text")
  data$text_title[i] <- stringr::str_match(text,"<title>(.+)</title>")[,2]
  data$text_content[i] <- stringr::str_match(text,"content=\"(.+).\"")[,2]
  auth_check <- stringr::str_match_all(text,"<h3 class=\"author__name\">(.*)</h3>")[[1]][,2]
  
  if (length(auth_check)>0){
    data$authors[i] <- auth_check
  }
  
  text <- stringr::str_extract_all(text,"<div class=\"rich-text\">.*</div>")[[1]]
  text <- unlist(stringr::str_extract_all(text,"<p>(.*)</p>"))
  text <- paste(unlist(stringr::str_split(text,"\\<(/|)p>")),collapse = "")
  text <- stringr::str_replace_all(text,"<(/|)i>", "")
  text <- stringr::str_replace_all(text,"<(/|)b>", "")
  text <- stringr::str_replace_all(text,"039", "'")
  text <- stringr::str_replace_all(text,"<a href=\"(.+?)\">","")
  text <- stringr::str_replace_all(text,"</a>","")
  
  data$text[i] <- text
  

  
  #make sure we are not downloading data too fast
  Sys.sleep(2)
  
}

